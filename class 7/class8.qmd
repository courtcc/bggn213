---
title: "Class 8 Mini-Project"
author: "Courtney Cameron PID: A69028599"
format: pdf
editor: visual
---

#Exploratory Data Analysis

Preparing the data

```{r}
fna.data<- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
#wisc.df
```

making the first column the rown names
```{r}
wisc.data <- wisc.df[,-1]
#wisc.data
```

creating vector based on the diagnosis
```{r}
diagnosis <- wisc.df[,1]
```

>Q1. How many observations are in this dataset?

The data has 569 rows and 30 columns resulting in 17070 data points

```{r}
dim(wisc.data)
569*30
```


>Q2. How many of the observations have a malignant diagnosis?

212 diagnosis' were malignant

```{r}
length(grep('M', diagnosis))
```

>Q3. How many variables/features in the data are suffixed with _mean?

9 variables have the suffix of _mean

```{r}
head(wisc.data)
```


#Principal component anlysis

checking if data needs to be scaled
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
```

```{r}
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

PC1 accounts for 44.27% of the variance

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

To describe at 70% of the original variance PC1-3 are needed 

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

To describe at 90% of the original variance PC1-7 are needed 

#Interpreting PCA resutlts

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot does not give a clear indication of groupings because the samples and their labels overlap too much to the point that no patterns can be discerend. All of the variables do project to the left but discerning which variable belongs to each vector is difficult due to the overlapping names being unreadable.

```{r}
biplot(wisc.pr)
```


PCA Scatter Plot
```{r}
plot( wisc.pr$x[,1],wisc.pr$x[,2], col=factor(diagnosis), 
     xlab = "PC1", ylab = "PC2")
```

```{r}
plot(wisc.pr$x[, 1 ],wisc.pr$x[,3], col = factor(diagnosis), 
     xlab = "PC1", ylab = "PC3")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

These two plots appear to follow a similar pattern with the black group being a tighter cluster on the right side of the plot and the red group being a more spread out grouping that spans further throughout the left side of the graph 

create the scatter plot using ggplot
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

library(ggplot2)

ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

#Variance Explained

calculate the variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/44.27


plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

plotting variance with ggplot
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

using the factoextra package
```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

The component of the loading vector for concave.points_mean is -0.26085376

```{r}
wisc.pr$rotation[,1]
```


#Hierarchical clustering

scaling the data
```{r}
data.scaled <- scale(wisc.data)
```
creating the distance matrix
```{r}
data.dist <- dist(data.scaled)
```
creating hierearchical clustering model with complete linkage method
```{r}
wisc.hclust <- hclust(data.dist, method='complete')
```

ploting the hclust results
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

>.Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?


the clustering model results in four clusters when the line is at a height of 19


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

>Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Based on the results from this data set, my preference for this data would probably be the ward.D2, because of the clear results it gave. The dendrogram shows three clear clusters while the other methods have more condensed crossbars that are more difficult to interpret and see a clustering pattern.

```{r}
hclust.single <- hclust(data.dist, method='single')
hclust.average <- hclust(data.dist, method='average')
wisc.pr.hclust<- hclust(data.dist, method='ward.D2')
```

ploting the hclust results
```{r}
plot(hclust.single)
plot(hclust.average)
plot(wisc.pr.hclust)

```
#Combining methods
clustering using ward.D2 clustering
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps,diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2],col=grps)
```
using a different call for color to swap the color groups
```{r}
plot(wisc.pr$x[,1:2], col=factor(diagnosis))
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g<-relevel(g,2)
levels(g)
```
```{r}
plot(wisc.pr$x[,1:2],col=g)
```

make a 3d plot - commented out becuase it doesn't work well when rendering a pdf
```{r}
#library(rgl)
#plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)

```

```{r}
wisc.pr.hclust <- (hclust(dist(wisc.pr$x[,1:7]), method='ward.D2'))
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust,k=2)
```

>Q13. How well does the newly created model with four clusters separate out the two diagnoses?

The four clusters seperate within the diganosis to have cluster 1 be favored with the malignant diagnosis while cluster 2 favors the benign diagnosis.

```{r}
table(wisc.pr.hclust.clusters,diagnosis )
```

>Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

when looking at the other methods the complete method does a good job of seperating the two diagnosis' into different clusters but with this data the single and average methods end up putting nearly all of the data points into one cluster with no separation based of diagnosis.

```{r}
table(wisc.hclust.clusters,diagnosis)
table(cutree(hclust.single,k=2),diagnosis)
table(cutree(hclust.average,k=2),diagnosis)
```

#Prediction

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q16. Which of these new patients should we prioritize for follow up based on your results?

Patient 2 should be prioritized because their information clusters them with the other malignant diagnoses. 


